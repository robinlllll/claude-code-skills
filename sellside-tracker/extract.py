#!/usr/bin/env python3
"""
Sellside Report Tracker â€” Phase 0 æç®€ç‰ˆ
PDF â†’ AI ç»“æ„åŒ–æå– â†’ YAML å­˜å‚¨ â†’ QoQ å¯¹æ¯”

Usage:
    python extract.py "path/to/report.pdf" [--ticker GOOG] [--quarter Q4-2025]
"""

import argparse
import csv
import json
import os
import re
import sys
from datetime import datetime
from pathlib import Path

if sys.platform == "win32":
    sys.stdout.reconfigure(encoding="utf-8", errors="replace")

import pymupdf
import yaml

# â”€â”€ Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SKILL_DIR = Path(__file__).parent
TEMPLATES_DIR = SKILL_DIR / "templates"
DATA_DIR = SKILL_DIR / "data"
DATA_DIR.mkdir(exist_ok=True)

sys.path.insert(0, str(Path.home() / ".claude" / "skills"))
from shared.obsidian_utils import create_note
from shared.dashboard_updater import update_dashboard

VAULT = Path.home() / "Documents" / "Obsidian Vault"


# â”€â”€ File Extraction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def extract_pdf_text(pdf_path: str) -> str:
    """Extract full text from PDF with page markers."""
    doc = pymupdf.open(pdf_path)
    pages = []
    for i, page in enumerate(doc):
        text = page.get_text()
        pages.append(f"[PAGE {i + 1}]\n{text}")
    return "\n\n".join(pages)


def extract_pptx_text(pptx_path: str) -> str:
    """Extract full text from PPTX with slide markers."""
    from pptx import Presentation
    prs = Presentation(pptx_path)
    slides = []
    for i, slide in enumerate(prs.slides):
        texts = []
        for shape in slide.shapes:
            if shape.has_text_frame:
                for para in shape.text_frame.paragraphs:
                    line = para.text.strip()
                    if line:
                        texts.append(line)
            if shape.has_table:
                table = shape.table
                for row in table.rows:
                    row_texts = [cell.text.strip() for cell in row.cells]
                    texts.append(" | ".join(row_texts))
        slides.append(f"[PAGE {i + 1}]\n" + "\n".join(texts))
    return "\n\n".join(slides)


def extract_file_text(file_path: str) -> str:
    """Extract text from PDF or PPTX."""
    ext = Path(file_path).suffix.lower()
    if ext == ".pdf":
        return extract_pdf_text(file_path)
    elif ext in (".pptx", ".ppt"):
        return extract_pptx_text(file_path)
    else:
        print(f"Error: Unsupported file type: {ext}", file=sys.stderr)
        sys.exit(1)


# â”€â”€ Template Loading â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

TICKER_ALIASES = {
    "GOOG": ["google", "alphabet"], "GOOGL": ["google", "alphabet"],
    "META": ["meta", "facebook"], "MSFT": ["microsoft"],
    "AAPL": ["apple"], "AMZN": ["amazon"],
    "NVDA": ["nvidia"], "TSLA": ["tesla"],
}


def load_template(ticker: str) -> list[dict]:
    """Load CSV template for a ticker. Tries ticker name then aliases."""
    candidates = [ticker.lower()]
    candidates.extend(TICKER_ALIASES.get(ticker.upper(), []))
    csv_path = None
    for name in candidates:
        p = TEMPLATES_DIR / f"{name}.csv"
        if p.exists():
            csv_path = p
            break
    if not csv_path:
        print(f"Error: No template found for {ticker} (tried: {candidates})", file=sys.stderr)
        sys.exit(1)
    with open(csv_path, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        return [row for row in reader]


def template_to_prompt_fields(fields: list[dict]) -> str:
    """Convert template fields to prompt instruction text."""
    lines = []
    for f in fields:
        req = " [å¿…å¡«]" if f["required"] == "yes" else ""
        note = f" ({f['notes']})" if f.get("notes", "").strip() else ""
        lines.append(f"- {f['key']} ({f['type']}): {f['label']}{note}{req}")
    return "\n".join(lines)


# â”€â”€ AI Extraction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_extraction_prompt(pdf_text: str, fields_text: str, ticker: str, quarter: str) -> str:
    return f"""ä½ æ˜¯é‡‘èæ•°æ®æå–åŠ©æ‰‹ã€‚ä»ä»¥ä¸‹å–æ–¹åˆ†æå¸ˆå­£åº¦æŠ¥å‘Šä¸­ç²¾ç¡®æå–æŒ‡æ ‡ã€‚

å…¬å¸: {ticker}
å­£åº¦: {quarter}

## æå–è§„åˆ™
1. æ¯ä¸ªæŒ‡æ ‡æ ‡æ³¨æ¥æºé¡µç  source_pageï¼ˆæ•´æ•°ï¼‰
2. æŠ¥å‘Šä¸­æœªæåŠçš„æŒ‡æ ‡è®¾ä¸º nullï¼Œç»ä¸çŒœæµ‹
3. éœ€è¦è®¡ç®—å¾—å‡ºçš„å€¼ï¼Œæ ‡æ³¨ confidence: calculated
4. percent ç±»å‹ä¿ç•™åŸå§‹æ ¼å¼å¦‚ "17%"
5. number ç±»å‹ç»Ÿä¸€ä¸ºç™¾ä¸‡ç¾å…ƒ(mn USD)ï¼Œè‹¥åŸæ–‡æ˜¯äº¿äººæ°‘å¸è¯·åœ¨ note å­—æ®µæ³¨æ˜
6. text ç±»å‹ç”¨åŸæ–‡å…³é”®å¥ï¼Œä¸è¶…è¿‡ 80 å­—
7. [å¿…å¡«] æ ‡è®°çš„å­—æ®µåŠ¡å¿…å°½åŠ›æå–

## è¦æå–çš„æŒ‡æ ‡
{fields_text}

## è¾“å‡ºæ ¼å¼
ä¸¥æ ¼è¾“å‡º YAMLï¼Œä¸è¦ markdown ä»£ç å—æ ‡è®°ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

ticker: {ticker}
quarter: {quarter}
report_date: YYYY-MM
source: "åˆ†æå¸ˆ/æœºæ„å"
metrics:
  <key>:
    value: <å€¼>
    source_page: <é¡µç >
    confidence: high|medium|calculated
    note: <å¯é€‰å¤‡æ³¨>

## æŠ¥å‘Šå…¨æ–‡
{pdf_text}
"""


def build_summary_prompt(pdf_text: str, ticker: str, quarter: str) -> str:
    return f"""ä½ æ˜¯ä¸€ä½èµ„æ·±ä¹°æ–¹åˆ†æå¸ˆåŠ©ç†ã€‚è¯·å°†ä»¥ä¸‹å–æ–¹å­£åº¦è·Ÿè¸ªæŠ¥å‘Šè½¬åŒ–ä¸ºç»“æ„åŒ–çš„æŠ•èµ„ç¬”è®°ã€‚

å…¬å¸: {ticker}
å­£åº¦: {quarter}

## è¾“å‡ºè¦æ±‚
ç”¨ä¸­æ–‡è¾“å‡º Markdown æ ¼å¼ï¼ˆä¸è¦ä»£ç å—åŒ…è£¹ï¼‰ï¼ŒæŒ‰ä»¥ä¸‹ç»“æ„ç»„ç»‡ï¼š

### æ ¸å¿ƒè§‚ç‚¹
- ç”¨ 3-5 ä¸ª bullet æ¦‚æ‹¬å–æ–¹æœ¬å­£åº¦çš„æ ¸å¿ƒåˆ¤æ–­ï¼ˆå¤šç©ºæ€åº¦ã€å…³é”®å˜åŒ–ã€ä¼°å€¼ç»“è®ºï¼‰

### Search å¹¿å‘Š
- æ”¶å…¥å¢é€Ÿã€é‡ä»·æ‹†åˆ†ï¼ˆclicks vs CPCï¼‰
- AI å¯¹æœç´¢çš„å½±å“ï¼ˆAI Overviewsã€AI Mode æ¸—é€ç‡ã€query å˜åŒ–ï¼‰
- ä¸‰æ–¹æ•°æ®éªŒè¯ï¼ˆå¦‚æœ‰ SimilarWebã€Yipit ç­‰ï¼‰
- ç«äº‰æ ¼å±€ï¼ˆä»½é¢å˜åŒ–ã€ChatGPT/Perplexity å†²å‡»ï¼‰

### Cloud / GCP
- æ”¶å…¥å¢é€Ÿã€åˆ©æ¶¦ç‡å˜åŒ–
- å®¢æˆ·è·å–ï¼ˆæ–°å®¢æˆ·ã€å¤§å•ã€backlogï¼‰
- AI äº‘äº§å“æ¸—é€ç‡
- Capex è§„æ¨¡åŠæŠ•å…¥æ–¹å‘

### YouTube & è®¢é˜…
- å¹¿å‘Šæ”¶å…¥å¢é€Ÿ
- Shorts è´§å¸åŒ–ã€CTV ä»½é¢
- è®¢é˜…ä¸šåŠ¡è§„æ¨¡ï¼ˆGoogle Oneã€Premiumã€TVï¼‰

### AI ç”Ÿæ€
- Gemini æ¨¡å‹è¿›å±•ï¼ˆMAUã€tokens å¤„ç†é‡ã€æ–°æ¨¡å‹å‘å¸ƒï¼‰
- AI agent å¹³å°ï¼ˆAntiGravityã€Genie ç­‰ï¼‰
- å†…éƒ¨ AI æ•ˆç‡æå‡ï¼ˆä»£ç ç”Ÿæˆã€è¿è¥è‡ªåŠ¨åŒ–ï¼‰

### å…¬å¸è´¢åŠ¡ & èµ„æœ¬é…ç½®
- æ”¶å…¥å¢é€Ÿã€OP å¢é€Ÿã€åˆ©æ¶¦ç‡å˜åŒ–
- è´¹ç”¨ç»“æ„ï¼ˆR&Dã€S&M æ æ†æ•ˆåº”ï¼‰
- Capex æŒ‡å¼•ã€FCF å½±å“
- å›è´­/åˆ†çº¢åŠ›åº¦å˜åŒ–

### ä¼°å€¼
- å–æ–¹çš„ä¼°å€¼æ¡†æ¶ï¼ˆæ•´ä½“ PEã€SOTP æ‹†åˆ†ï¼‰
- å„ä¸šåŠ¡éšå«ä¼°å€¼ï¼ˆSearchã€Cloudã€YouTubeã€Waymoï¼‰
- å–æ–¹ vs å¸‚åœºä¸€è‡´é¢„æœŸçš„å·®å¼‚ï¼ˆå¦‚æœ‰ï¼‰

### Other Bets
- Waymoï¼ˆèèµ„ã€ridesã€åŸå¸‚æ‰©å±•ï¼‰
- DeepMind è¿›å±•

## è§„åˆ™
1. ä¿ç•™å…·ä½“æ•°å­—å’Œç™¾åˆ†æ¯”ï¼Œæ ‡æ³¨æ¥æºé¡µç å¦‚ (p.5)
2. åŒºåˆ†å…¬å¸æŠ«éœ²æ•°æ®å’Œå–æ–¹è‡ªå·±çš„æ¨ç®—/åˆ¤æ–­
3. å¦‚æœæŸä¸ª section æŠ¥å‘Šä¸­æœªæ¶‰åŠï¼Œå†™"æœ¬æŠ¥å‘Šæœªæ¶‰åŠ"
4. ä¸è¦æ·»åŠ æŠ¥å‘Šä¸­æ²¡æœ‰çš„ä¿¡æ¯

## æŠ¥å‘Šå…¨æ–‡
{pdf_text}
"""


def call_gemini(prompt: str) -> str:
    """Call Gemini API for extraction."""
    from google import genai

    # Load API key
    config_path = Path.home() / ".claude" / "skills" / "prompt-optimizer" / "data" / "config.json"
    if config_path.exists():
        cfg = json.loads(config_path.read_text(encoding="utf-8"))
        api_key = cfg.get("GEMINI_API_KEY", "")
    else:
        api_key = os.environ.get("GEMINI_API_KEY", "")

    if not api_key:
        print("Error: No Gemini API key found.", file=sys.stderr)
        sys.exit(1)

    client = genai.Client(api_key=api_key)
    response = client.models.generate_content(
        model="gemini-2.0-flash",
        contents=prompt,
        config=genai.types.GenerateContentConfig(
            temperature=0.1,
            max_output_tokens=4096,
        ),
    )
    return response.text


# â”€â”€ Validation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def validate_extraction(data: dict, fields: list[dict], prev_data: dict | None) -> list[str]:
    """Validate extracted data. Returns list of warnings."""
    warnings = []
    metrics = data.get("metrics", {})

    for f in fields:
        key = f["key"]
        m = metrics.get(key)

        # Required field missing
        if f["required"] == "yes" and (m is None or m.get("value") is None):
            warnings.append(f"âš ï¸ å¿…å¡«å­—æ®µç¼ºå¤±: {f['label']} ({key})")
            continue

        if m is None or m.get("value") is None:
            continue

        val = m["value"]

        # Range check for percents
        if f["type"] == "percent" and isinstance(val, str) and "%" in val:
            try:
                num = float(val.replace("%", "").strip())
                if abs(num) > 200:
                    warnings.append(f"âš ï¸ å¼‚å¸¸å€¼: {f['label']} = {val}")
            except ValueError:
                pass

        # QoQ deviation check
        if prev_data and key in prev_data.get("metrics", {}):
            prev_val = prev_data["metrics"][key].get("value")
            if prev_val and f["type"] == "percent":
                try:
                    cur = float(str(val).replace("%", "").strip())
                    prev = float(str(prev_val).replace("%", "").strip())
                    delta = abs(cur - prev)
                    if delta > 20:
                        warnings.append(
                            f"âš ï¸ å¤§å¹…å˜åŒ–: {f['label']} {prev_val} â†’ {val} (Î”{delta:.0f}pp)"
                        )
                except ValueError:
                    pass

    return warnings


# â”€â”€ Storage â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_history(ticker: str) -> dict:
    """Load all historical quarterly data for a ticker."""
    path = DATA_DIR / f"{ticker.lower()}_quarterly.yaml"
    if path.exists():
        with open(path, "r", encoding="utf-8") as f:
            return yaml.safe_load(f) or {}
    return {}


def save_history(ticker: str, history: dict):
    """Save quarterly data for a ticker."""
    path = DATA_DIR / f"{ticker.lower()}_quarterly.yaml"
    with open(path, "w", encoding="utf-8") as f:
        yaml.dump(history, f, allow_unicode=True, default_flow_style=False, sort_keys=False)
    print(f"âœ… æ•°æ®å·²ä¿å­˜: {path}")


def get_previous_quarter(history: dict, current_quarter: str) -> dict | None:
    """Get the most recent quarter before current."""
    quarters = sorted(history.keys())
    if current_quarter in quarters:
        idx = quarters.index(current_quarter)
        if idx > 0:
            return history[quarters[idx - 1]]
    elif quarters:
        return history[quarters[-1]]
    return None


# â”€â”€ QoQ Comparison â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def generate_comparison(current: dict, previous: dict | None, fields: list[dict]) -> str:
    """Generate QoQ comparison markdown table."""
    if not previous:
        return "_é¦–æ¬¡æå–ï¼Œæ— å†å²æ•°æ®å¯å¯¹æ¯”_"

    lines = [
        f"## {current['ticker']} {current['quarter']} vs {previous['quarter']}",
        "",
        "| æŒ‡æ ‡ | ä¸Šå­£ | æœ¬å­£ | å˜åŒ– | ä¿¡å· |",
        "|------|------|------|------|------|",
    ]

    cur_m = current.get("metrics", {})
    prev_m = previous.get("metrics", {})

    for f in fields:
        key = f["key"]
        if f["type"] == "text":
            continue  # Skip text fields in comparison table

        cur_val = cur_m.get(key, {}).get("value") if key in cur_m else None
        prev_val = prev_m.get(key, {}).get("value") if key in prev_m else None

        if cur_val is None and prev_val is None:
            continue

        cur_str = str(cur_val) if cur_val is not None else "â€”"
        prev_str = str(prev_val) if prev_val is not None else "â€”"

        # Calculate delta for percent/number
        signal = ""
        delta_str = ""
        if cur_val is not None and prev_val is not None:
            try:
                if f["type"] == "percent":
                    c = float(str(cur_val).replace("%", "").strip())
                    p = float(str(prev_val).replace("%", "").strip())
                    d = c - p
                    delta_str = f"{d:+.1f}pp"
                    signal = "ğŸŸ¢ åŠ é€Ÿ" if d > 2 else ("ğŸ”´ å‡é€Ÿ" if d < -2 else "â€”")
                elif f["type"] == "number":
                    c = float(str(cur_val).replace(",", ""))
                    p = float(str(prev_val).replace(",", ""))
                    if p != 0:
                        pct = (c - p) / abs(p) * 100
                        delta_str = f"{pct:+.1f}%"
                        signal = "ğŸŸ¢" if pct > 5 else ("ğŸ”´" if pct < -5 else "â€”")
            except (ValueError, TypeError):
                delta_str = "â€”"

        if cur_val is None:
            signal = "âš ï¸ ç¼ºå¤±"
        elif prev_val is None:
            signal = "ğŸ†•"

        lines.append(f"| {f['label']} | {prev_str} | {cur_str} | {delta_str} | {signal} |")

    return "\n".join(lines)


# â”€â”€ Obsidian Output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def save_to_obsidian(current: dict, comparison: str, fields: list[dict], report_summary: str = ""):
    """Save extraction result, full summary, and comparison to Obsidian vault."""
    ticker = current["ticker"]
    quarter = current["quarter"]
    today = datetime.now().strftime("%Y-%m-%d")

    # Build note content â€” structure: summary first, then QoQ, then raw KPIs
    metrics = current.get("metrics", {})

    # Group metrics by segment for the KPI appendix
    segment_data = {}
    for f in fields:
        seg = f["segment"]
        if seg not in segment_data:
            segment_data[seg] = []
        m = metrics.get(f["key"], {})
        val = m.get("value") if m else None
        page = m.get("source_page") if m else None
        segment_data[seg].append({
            "label": f["label"],
            "value": val,
            "page": page,
        })

    body_parts = []

    # Part 1: Full report summary (the rich content)
    if report_summary:
        body_parts.append(report_summary.strip())
        body_parts.append("")

    # Part 2: QoQ comparison table
    body_parts.extend(["---", "", "## QoQ å¯¹æ¯”", ""])
    body_parts.append(comparison)

    # Part 3: Raw KPI appendix (collapsible)
    body_parts.extend(["", "---", "", "<details>", "<summary>ğŸ“Š KPI æå–æ˜ç»†ï¼ˆç‚¹å‡»å±•å¼€ï¼‰</summary>", ""])
    for seg, items in segment_data.items():
        body_parts.append(f"#### {seg}")
        body_parts.append("| æŒ‡æ ‡ | å€¼ | é¡µç  |")
        body_parts.append("|------|-----|------|")
        for item in items:
            v = str(item["value"]) if item["value"] is not None else "â€”"
            p = str(item["page"]) if item["page"] is not None else "â€”"
            body_parts.append(f"| {item['label']} | {v} | p.{p} |")
        body_parts.append("")
    body_parts.append("</details>")

    body_parts.extend([
        "",
        "---",
        f"_æ¥æº: {current.get('source', 'N/A')} | æå–æ—¶é—´: {today}_",
    ])

    frontmatter = {
        "tags": ["å–æ–¹è·Ÿè¸ª", ticker],
        "ticker": ticker,
        "quarter": quarter,
        "source": current.get("source", ""),
        "date": today,
    }

    fm_yaml = yaml.dump(frontmatter, allow_unicode=True, default_flow_style=False).strip()
    content = f"---\n{fm_yaml}\n---\n\n# {ticker} {quarter} å–æ–¹è·Ÿè¸ª\n\n" + "\n".join(body_parts)

    # Save (overwrite if exists)
    rel_path = f"ç ”ç©¶/å–æ–¹è·Ÿè¸ª/{ticker}/{today} {ticker} {quarter} å–æ–¹è·Ÿè¸ª.md"
    full_path = VAULT / rel_path
    full_path.parent.mkdir(parents=True, exist_ok=True)
    with open(full_path, "w", encoding="utf-8") as f:
        f.write(content)
    print(f"âœ… Obsidian ç¬”è®°å·²ä¿å­˜: {rel_path}")

    try:
        update_dashboard()
        print("âœ… Dashboard å·²æ›´æ–°")
    except Exception as e:
        print(f"âš ï¸ Dashboard æ›´æ–°å¤±è´¥: {e}")


# â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def detect_ticker_from_filename(filename: str) -> str | None:
    """Try to detect ticker from filename."""
    name = Path(filename).stem.lower()
    mapping = {
        "google": "GOOG", "alphabet": "GOOG",
        "meta": "META", "facebook": "META",
        "microsoft": "MSFT", "msft": "MSFT",
        "apple": "AAPL",
        "amazon": "AMZN", "amzn": "AMZN",
        "nvidia": "NVDA",
        "tesla": "TSLA",
    }
    for keyword, ticker in mapping.items():
        if keyword in name:
            return ticker
    return None


def detect_quarter_from_filename(filename: str) -> str | None:
    """Try to detect quarter from filename, e.g. '25Q4' -> 'Q4-2025'."""
    name = Path(filename).stem
    # Match patterns like 25Q4, Q4 2025, 2025Q4, etc.
    m = re.search(r"(\d{2,4})\s*Q(\d)", name, re.IGNORECASE)
    if m:
        year = m.group(1)
        q = m.group(2)
        if len(year) == 2:
            year = "20" + year
        return f"Q{q}-{year}"
    m = re.search(r"Q(\d)\s*(\d{2,4})", name, re.IGNORECASE)
    if m:
        q = m.group(1)
        year = m.group(2)
        if len(year) == 2:
            year = "20" + year
        return f"Q{q}-{year}"
    return None


def main():
    parser = argparse.ArgumentParser(description="Sellside Report Tracker â€” Phase 0")
    parser.add_argument("pdf", help="Path to sellside report (PDF or PPTX)")
    parser.add_argument("--ticker", help="Ticker symbol (auto-detected if omitted)")
    parser.add_argument("--quarter", help="Quarter like Q4-2025 (auto-detected if omitted)")
    parser.add_argument("--no-obsidian", action="store_true", help="Skip Obsidian save")
    parser.add_argument("--dry-run", action="store_true", help="Extract only, don't save")
    args = parser.parse_args()

    pdf_path = args.pdf
    if not os.path.exists(pdf_path):
        print(f"Error: File not found: {pdf_path}", file=sys.stderr)
        sys.exit(1)

    # Detect ticker & quarter
    ticker = args.ticker or detect_ticker_from_filename(pdf_path)
    quarter = args.quarter or detect_quarter_from_filename(pdf_path)

    if not ticker:
        print("Error: Cannot detect ticker. Use --ticker GOOG", file=sys.stderr)
        sys.exit(1)
    if not quarter:
        print("Error: Cannot detect quarter. Use --quarter Q4-2025", file=sys.stderr)
        sys.exit(1)

    ticker = ticker.upper()
    print(f"ğŸ“„ PDF: {Path(pdf_path).name}")
    print(f"ğŸ·ï¸  Ticker: {ticker} | Quarter: {quarter}")

    # Load template
    fields = load_template(ticker)
    print(f"ğŸ“‹ æ¨¡æ¿: {len(fields)} ä¸ªæŒ‡æ ‡ ({TEMPLATES_DIR / f'{ticker.lower()}.csv'})")

    # Extract PDF text
    print("ğŸ“– æå– PDF æ–‡æœ¬...")
    pdf_text = extract_file_text(pdf_path)
    page_count = pdf_text.count("[PAGE ")
    print(f"   {page_count} é¡µï¼Œ{len(pdf_text)} å­—ç¬¦")

    # Build prompt & call AI â€” two calls: KPI extraction + full summary
    fields_text = template_to_prompt_fields(fields)
    prompt = build_extraction_prompt(pdf_text, fields_text, ticker, quarter)
    summary_prompt = build_summary_prompt(pdf_text, ticker, quarter)
    print(f"ğŸ¤– è°ƒç”¨ Gemini: KPI æå– + å…¨æ–‡æ‘˜è¦...")
    raw_response = call_gemini(prompt)
    print(f"   âœ… KPI æå–å®Œæˆ")
    print(f"   ğŸ“ ç”Ÿæˆå…¨æ–‡æ‘˜è¦ä¸­...")
    report_summary = call_gemini(summary_prompt)
    print(f"   âœ… å…¨æ–‡æ‘˜è¦å®Œæˆ ({len(report_summary)} chars)")

    # Parse YAML response
    # Strip markdown code fence if present
    cleaned = raw_response.strip()
    if cleaned.startswith("```"):
        cleaned = re.sub(r"^```\w*\n?", "", cleaned)
        cleaned = re.sub(r"\n?```$", "", cleaned)

    try:
        data = yaml.safe_load(cleaned)
    except yaml.YAMLError as e:
        print(f"âŒ YAML è§£æå¤±è´¥: {e}", file=sys.stderr)
        print("--- Raw response ---")
        print(raw_response[:2000])
        sys.exit(1)

    # Count extracted metrics
    metrics = data.get("metrics", {})
    filled = sum(1 for m in metrics.values() if m and m.get("value") is not None)
    total = len(fields)
    print(f"âœ… æå–å®Œæˆ: {filled}/{total} æŒ‡æ ‡å·²å¡«å……")

    # Load history & validate
    history = load_history(ticker)
    prev = get_previous_quarter(history, quarter)
    warnings = validate_extraction(data, fields, prev)

    if warnings:
        print(f"\nâš ï¸ {len(warnings)} ä¸ªè­¦å‘Š:")
        for w in warnings:
            print(f"  {w}")

    # Print summary table
    print(f"\n{'â”€' * 60}")
    print(f"{'æŒ‡æ ‡':<30} {'å€¼':<20} {'é¡µç ':<5}")
    print(f"{'â”€' * 60}")
    for f in fields:
        key = f["key"]
        m = metrics.get(key, {})
        val = m.get("value", "â€”") if m else "â€”"
        page = m.get("source_page", "â€”") if m else "â€”"
        label = f["label"][:28]
        print(f"{label:<30} {str(val):<20} p.{page}")
    print(f"{'â”€' * 60}")

    if args.dry_run:
        print("\nğŸ Dry run å®Œæˆï¼Œæœªä¿å­˜æ•°æ®")
        return

    # Save to history
    history[quarter] = data
    save_history(ticker, history)

    # Generate comparison
    comparison = generate_comparison(data, prev, fields)
    print(f"\n{comparison}")

    # Save to Obsidian
    if not args.no_obsidian:
        save_to_obsidian(data, comparison, fields, report_summary)

    print(f"\nğŸ å®Œæˆï¼{filled}/{total} æŒ‡æ ‡ | {len(warnings)} è­¦å‘Š")


if __name__ == "__main__":
    main()
