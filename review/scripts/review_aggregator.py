"""Review Aggregator â€” generates structured weekly/monthly review for /review skill.

Aggregates 10 data sources into a single Obsidian-formatted markdown review file.

Usage:
    python review_aggregator.py --period week [--days 7]
    python review_aggregator.py --period month [--days 30]
    python review_aggregator.py --dry-run  # print to stdout, don't save

Data sources:
    1. Portfolio trades (PM API / SQLite)
    2. Research Notes (Obsidian)
    3. Earnings Analysis (Obsidian)
    4. Thesis updates (thesis.md/yaml mtime)
    5. å‘¨ä¼š (Obsidian)
    6. Weekly Inbox (Obsidian)
    7. Podcast (Obsidian)
    8. BiasEngine dashboard (PM API)
    9. Kill Criteria scan (thesis.yaml)
    10. Decision Journal stats (decision_stats.py)
"""

import argparse
import asyncio
import json
import re
import subprocess
import sys
from datetime import datetime, timedelta
from pathlib import Path

import httpx
import yaml

# â”€â”€ Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

HOME = Path.home()
PORTFOLIO_API = "http://localhost:8000"
PORTFOLIO_DB = HOME / "PORTFOLIO" / "portfolio_monitor" / "data" / "portfolio.db"
THESIS_DIR = HOME / "PORTFOLIO" / "research" / "companies"
VAULT = HOME / "Documents" / "Obsidian Vault"
REVIEWS_DIR = VAULT / "å†™ä½œ" / "æŠ•èµ„å›é¡¾"
ESTIMATES_DIR = VAULT / "ç ”ç©¶" / "ä¼°å€¼"
JOURNAL_DIR = HOME / "PORTFOLIO" / "decisions" / "journal"

# Obsidian data source folders (restructured 2026-02-07)
RESEARCH_NOTES = VAULT / "ç ”ç©¶" / "ç ”ç©¶ç¬”è®°"
EARNINGS_ANALYSIS = VAULT / "ç ”ç©¶" / "è´¢æŠ¥åˆ†æ"
ZHOUHUI = VAULT / "å‘¨ä¼š"
WEEKLY_INBOX = VAULT / "æ”¶ä»¶ç®±"
PODCAST = VAULT / "ä¿¡æ¯æº" / "æ’­å®¢"

REVIEWS_DIR.mkdir(parents=True, exist_ok=True)

PYTHON = str(HOME / "AppData" / "Local" / "Python" / "pythoncore-3.14-64" / "python.exe")
if not Path(PYTHON).exists():
    PYTHON = sys.executable

DECISION_STATS = Path(__file__).parent / "decision_stats.py"
ESTIMATE_STATS = Path(__file__).parent / "estimate_stats.py"


# â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def files_in_period(folder: Path, days: int, extensions: tuple = (".md",)) -> list[Path]:
    """Find files date-prefixed within the lookback window.

    Uses date prefix in filename (YYYY-MM-DD) as primary filter.
    Falls back to mtime only for files without date prefixes AND only if
    the file was recently created (not just synced/touched).
    """
    if not folder.exists():
        return []
    cutoff = datetime.now() - timedelta(days=days)
    cutoff_str = cutoff.strftime("%Y-%m-%d")
    results = []
    for f in folder.rglob("*"):
        if f.suffix not in extensions or f.name.startswith("TEMPLATE"):
            continue
        # Try date prefix first (YYYY-MM-DD)
        match = re.match(r"(\d{4}-\d{2}-\d{2})", f.name)
        if match:
            if match.group(1) >= cutoff_str:
                results.append(f)
            continue  # Always skip mtime fallback if file has date prefix
        # Fall back to creation time (not mtime, which Syncthing updates)
        try:
            ctime = datetime.fromtimestamp(f.stat().st_ctime)
            if ctime >= cutoff:
                results.append(f)
        except Exception:
            pass
    results.sort(key=lambda p: p.name)
    return results


# â”€â”€ Data Source Fetchers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def fetch_trades(days: int) -> str:
    """Source 1: Recent trades from PM API."""
    try:
        async with httpx.AsyncClient(timeout=10) as client:
            resp = await client.get(f"{PORTFOLIO_API}/api/trades")
            if resp.status_code != 200:
                return "_Portfolio Monitor API ä¸å¯ç”¨_\n"
            data = resp.json()
            trades = data.get("trades", [])
    except Exception:
        # SQLite fallback
        try:
            import sqlite3
            conn = sqlite3.connect(str(PORTFOLIO_DB))
            conn.row_factory = sqlite3.Row
            cutoff = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")
            cur = conn.execute(
                "SELECT * FROM trades WHERE entry_date >= ? ORDER BY entry_date DESC", (cutoff,)
            )
            trades = [dict(r) for r in cur.fetchall()]
            conn.close()
        except Exception as e:
            return f"_äº¤æ˜“æ•°æ®ä¸å¯ç”¨: {e}_\n"

    cutoff = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")
    recent = [t for t in trades if (t.get("entry_date") or "") >= cutoff]

    if not recent:
        return "æœ¬æœŸæ— äº¤æ˜“ã€‚\n"

    lines = ["| æ—¥æœŸ | æ–¹å‘ | Ticker | æ•°é‡ | ä»·æ ¼ |",
             "|------|------|--------|------|------|"]
    for t in recent[:20]:
        lines.append(
            f"| {t.get('entry_date', '?')} | {t.get('direction', '?')} | "
            f"{t.get('ticker', '?')} | {t.get('quantity', 0)} | "
            f"${t.get('entry_price', 0):.2f} |"
        )
    if len(recent) > 20:
        lines.append(f"_... +{len(recent) - 20} more_")
    return "\n".join(lines) + "\n"


def fetch_obsidian_files(folder: Path, label: str, days: int) -> str:
    """Sources 2-7: List recent Obsidian files from a folder."""
    files = files_in_period(folder, days)
    if not files:
        return f"æœ¬æœŸæ— {label}ã€‚\n"
    lines = []
    for f in files[:15]:
        name = f.stem
        rel = f.relative_to(VAULT) if str(f).startswith(str(VAULT)) else f.name
        lines.append(f"- [[{name}]]")
    if len(files) > 15:
        lines.append(f"_... +{len(files) - 15} more_")
    return "\n".join(lines) + "\n"


async def fetch_bias_dashboard(days: int) -> str:
    """Source 8: BiasEngine dashboard."""
    try:
        async with httpx.AsyncClient(timeout=10) as client:
            resp = await client.get(
                f"{PORTFOLIO_API}/api/bias/dashboard",
                params={"lookback_days": days},
            )
            if resp.status_code != 200:
                return "_BiasEngine API ä¸å¯ç”¨_\n"
            data = resp.json()
    except Exception:
        return "_BiasEngine API ä¸å¯ç”¨ (Portfolio Monitor æœªè¿è¡Œ?)_\n"

    # Parse dashboard
    overall = data.get("overall_health", {})
    biases = data.get("biases", {})

    lines = [
        f"**Overall Health:** {overall.get('label', '?')} ({overall.get('score', '?')})\n",
        "| Bias | Severity | Confidence |",
        "|------|----------|------------|",
    ]

    # Flatten biases across instrument types
    seen = set()
    for inst_type, bias_list in biases.items():
        if isinstance(bias_list, list):
            for b in bias_list:
                bias_id = b.get("bias_id", "?")
                if bias_id in seen:
                    continue
                seen.add(bias_id)
                score = b.get("severity_score", 0)
                icon = "ğŸŸ¢" if score < 50 else "ğŸŸ¡" if score < 70 else "ğŸ”´"
                lines.append(
                    f"| {b.get('display_name', bias_id)} | {score:.0f} {icon} | "
                    f"{b.get('confidence', '?')} |"
                )

    # Flagged episodes
    insights = data.get("insights", {})
    multi_flagged = insights.get("multi_flagged_episodes", [])
    if multi_flagged:
        lines.append(f"\n**å¤šé‡è§¦å‘:** {len(multi_flagged)} ä¸ª episode è¢«å¤šä¸ªæ£€æµ‹å™¨æ ‡è®°")

    return "\n".join(lines) + "\n"


def scan_kill_criteria(days: int) -> str:
    """Source 9: Kill Criteria scan across all thesis.yaml files."""
    if not THESIS_DIR.exists():
        return "_No thesis directory_\n"

    cutoff = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")
    overdue_threshold_quant = 30  # days
    overdue_threshold_qual = 14  # days

    all_tickers = []
    overdue = []
    warnings = []
    no_kc = []
    violations = []  # KC fail_detected_at > 48h unresolved

    for d in sorted(THESIS_DIR.iterdir()):
        if not d.is_dir():
            continue
        yf = d / "thesis.yaml"
        if not yf.exists():
            continue
        try:
            data = yaml.safe_load(yf.read_text(encoding="utf-8"))
        except Exception:
            continue

        ticker = d.name
        kcs = data.get("kill_criteria", [])

        if not kcs:
            no_kc.append(ticker)
            all_tickers.append({"ticker": ticker, "total": 0, "pass": 0, "warning": 0, "fail": 0, "unchecked": 0})
            continue

        stats = {"ticker": ticker, "total": len(kcs), "pass": 0, "warning": 0, "fail": 0, "unchecked": 0}
        for kc in kcs:
            result = kc.get("check_result", "unchecked")
            stats[result] = stats.get(result, 0) + 1

            # Check overdue
            last_checked = kc.get("last_checked", "")
            if last_checked:
                try:
                    checked_date = datetime.strptime(str(last_checked), "%Y-%m-%d")
                    age_days = (datetime.now() - checked_date).days
                    kc_type = kc.get("type", "quantitative")
                    threshold = overdue_threshold_qual if kc_type == "qualitative" else overdue_threshold_quant
                    if age_days > threshold:
                        overdue.append({
                            "ticker": ticker,
                            "condition": kc.get("condition", "?")[:50],
                            "last_checked": last_checked,
                            "days": age_days,
                        })
                except ValueError:
                    pass

            if result == "warning":
                warnings.append({"ticker": ticker, "condition": kc.get("condition", "?")[:50]})
            elif result == "fail":
                warnings.append({"ticker": ticker, "condition": f"FAIL: {kc.get('condition', '?')[:50]}"})

            # Check for discipline violations: fail_detected_at > 48h
            fail_at = kc.get("fail_detected_at")
            if fail_at:
                try:
                    if isinstance(fail_at, str):
                        fail_dt = datetime.fromisoformat(fail_at)
                    elif isinstance(fail_at, datetime):
                        fail_dt = fail_at
                    else:
                        fail_dt = None
                    if fail_dt:
                        hours_since = (datetime.now() - fail_dt).total_seconds() / 3600
                        if hours_since > 48:
                            violations.append({
                                "ticker": ticker,
                                "condition": kc.get("condition", "?")[:60],
                                "hours": round(hours_since),
                            })
                except (ValueError, TypeError):
                    pass

        all_tickers.append(stats)

    lines = []

    # Discipline Violations (highest priority â€” shown first)
    if violations:
        lines.append("### ğŸ”´ DISCIPLINE VIOLATIONS â€” çºªå¾‹è¿è§„")
        lines.append("")
        lines.append("> Kill criteria è§¦å‘è¶…è¿‡ 48 å°æ—¶æœªå¤„ç†ã€‚å¿…é¡»ç«‹å³è¡ŒåŠ¨ã€‚")
        lines.append("")
        lines.append("| Ticker | Failed Condition | Hours Unresolved |")
        lines.append("|--------|-----------------|------------------|")
        for v in violations:
            lines.append(f"| **{v['ticker']}** | {v['condition']} | {v['hours']}h |")
        lines.append("")

    # Overdue section
    if overdue:
        lines.append("### éœ€è¦ç«‹å³æ£€æŸ¥ï¼ˆè¿‡æœŸï¼‰")
        lines.append("| Ticker | Condition | Last Checked | Days |")
        lines.append("|--------|-----------|--------------|------|")
        for o in overdue:
            lines.append(f"| {o['ticker']} | {o['condition']} | {o['last_checked']} | {o['days']}d |")
        lines.append("")

    # Warnings
    if warnings:
        lines.append("### Warning/Fail æ¡ä»¶")
        for w in warnings:
            lines.append(f"- **{w['ticker']}**: {w['condition']}")
        lines.append("")

    # Summary table
    lines.append("### å…¨éƒ¨æŒä»“ Kill Criteria æ€»è§ˆ")
    lines.append("| Ticker | Total | Pass | Warning | Fail | Unchecked |")
    lines.append("|--------|-------|------|---------|------|-----------|")
    for s in all_tickers:
        lines.append(
            f"| {s['ticker']} | {s['total']} | {s['pass']} | {s['warning']} | "
            f"{s['fail']} | {s['unchecked']} |"
        )
    lines.append("")

    # No KC warning
    if no_kc:
        lines.append("### æ—  Kill Criteria çš„æŒä»“")
        for t in no_kc:
            lines.append(f"- **{t}** â† æ²¡æœ‰å®šä¹‰é€€å‡ºæ¡ä»¶")
        lines.append("")

    return "\n".join(lines) + "\n"


def fetch_decision_stats(days: int) -> str:
    """Source 10: Decision Journal stats."""
    if DECISION_STATS.exists():
        try:
            result = subprocess.run(
                [PYTHON, str(DECISION_STATS), "--days", str(days), "--output", "markdown"],
                capture_output=True, text=True, timeout=10, encoding="utf-8",
            )
            if result.returncode == 0 and result.stdout.strip():
                return result.stdout.strip() + "\n"
        except Exception:
            pass
    return "_Decision Journal ç»Ÿè®¡ä¸å¯ç”¨_\n"


def fetch_estimate_stats(days: int) -> str:
    """Bonus: Estimate stats if available."""
    if ESTIMATE_STATS.exists():
        try:
            result = subprocess.run(
                [PYTHON, str(ESTIMATE_STATS), "--days", str(days), "--output", "markdown"],
                capture_output=True, text=True, timeout=10, encoding="utf-8",
            )
            if result.returncode == 0 and result.stdout.strip():
                return result.stdout.strip() + "\n"
        except Exception:
            pass
    return ""


def fetch_peers_reminder(days: int) -> str:
    """Source: Peers earnings/events reminder."""
    if not THESIS_DIR.exists():
        return ""

    peers_map = {}  # peer_ticker â†’ [holding_ticker, ...]
    for d in sorted(THESIS_DIR.iterdir()):
        if not d.is_dir():
            continue
        yf = d / "thesis.yaml"
        if not yf.exists():
            continue
        try:
            data = yaml.safe_load(yf.read_text(encoding="utf-8"))
        except Exception:
            continue
        for p in data.get("peers", []):
            peer_ticker = p.get("ticker", "")
            if peer_ticker:
                peers_map.setdefault(peer_ticker, []).append(d.name)

    if not peers_map:
        return ""

    # Check if any peer has recent earnings analysis
    recent_analysis = set()
    if EARNINGS_ANALYSIS.exists():
        for f in files_in_period(EARNINGS_ANALYSIS, days):
            # Extract ticker from folder name or filename
            for peer in peers_map:
                if peer.upper() in f.name.upper() or peer.upper() in str(f.parent).upper():
                    recent_analysis.add(peer)

    # Check å‘¨ä¼š mentions
    zhouhui_mentions = set()
    if ZHOUHUI.exists():
        for f in files_in_period(ZHOUHUI, days):
            try:
                content = f.read_text(encoding="utf-8")
                for peer in peers_map:
                    if peer.upper() in content.upper():
                        zhouhui_mentions.add(peer)
            except Exception:
                pass

    lines = []
    if recent_analysis or zhouhui_mentions or peers_map:
        lines.append("| ä½ çš„æŒä»“ | Peer | è¿‘æœŸåŠ¨æ€ |")
        lines.append("|---------|------|---------|")
        for peer, holdings in peers_map.items():
            events = []
            if peer in recent_analysis:
                events.append("æœ‰è´¢æŠ¥åˆ†æ")
            if peer in zhouhui_mentions:
                events.append("å‘¨ä¼šæåŠ")
            if not events:
                events.append("æœªå…³æ³¨")
            lines.append(f"| {', '.join(holdings)} | {peer} | {' + '.join(events)} |")

    return "\n".join(lines) + "\n" if lines else ""


async def compute_sizing_deviations() -> str:
    """Compute position sizing vs actual for all tickers with sizing data."""
    if not THESIS_DIR.exists():
        return ""

    # Get actual portfolio weights
    actual_weights = {}
    try:
        async with httpx.AsyncClient(timeout=10) as client:
            resp = await client.get(f"{PORTFOLIO_API}/api/portfolio")
            if resp.status_code == 200:
                data = resp.json()
                nav = data.get("total_value", 0)
                if nav > 0:
                    for pos in data.get("positions", []):
                        ticker = pos.get("symbol", "").split()[0].upper()
                        mkt_val = abs(float(pos.get("market_value", 0)))
                        actual_weights[ticker] = mkt_val / nav * 100
    except Exception:
        pass

    conv_mult = {1: 0.5, 2: 0.75, 3: 1.0, 4: 1.5, 5: 2.0}
    qual_mult = {"A": 1.2, "B": 1.0, "C": 0.7}

    rows = []
    for d in sorted(THESIS_DIR.iterdir()):
        if not d.is_dir():
            continue
        yf = d / "thesis.yaml"
        if not yf.exists():
            continue
        try:
            data = yaml.safe_load(yf.read_text(encoding="utf-8"))
        except Exception:
            continue

        ticker = d.name
        conviction = data.get("conviction", 3)
        quality = data.get("quality_grade", "C")
        base = data.get("base_size_pct", 5)

        suggested = min(
            base * conv_mult.get(conviction, 1.0) * qual_mult.get(quality, 1.0),
            10,
        )
        actual = actual_weights.get(ticker, 0)

        if actual > 0 or data.get("sizing_result_pct"):
            diff = actual - suggested
            icon = "âœ…" if abs(diff) < 2 else "âš ï¸"
            rows.append(
                f"| {ticker} | {suggested:.1f}% | {actual:.1f}% | {diff:+.1f}% {icon} |"
            )

    if not rows:
        return ""

    lines = [
        "| Ticker | å»ºè®®ä»“ä½ | å®é™…ä»“ä½ | åå·® |",
        "|--------|---------|---------|------|",
    ] + rows

    return "\n".join(lines) + "\n"


def fetch_attribution_summary() -> str:
    """Source 11: Research ROI from attribution_report.py."""
    try:
        skills_dir = HOME / ".claude" / "skills"
        sys.path.insert(0, str(skills_dir))
        from shared.attribution_report import (
            generate_attribution_report,
        )
        report = generate_attribution_report(save=False)
    except Exception as e:
        return f"_Attribution æ•°æ®ä¸å¯ç”¨: {e}_\n"

    # Extract the key sections: Source Efficiency Ranking + Conviction Calibration + Coverage vs Returns
    lines = []
    in_section = False
    target_headers = [
        "### Source Efficiency Ranking",
        "### Conviction Calibration",
        "### Coverage vs Returns",
    ]
    for line in report.split("\n"):
        if any(line.startswith(h) for h in target_headers):
            in_section = True
            lines.append(line)
            continue
        if in_section:
            if line.startswith("## ") or line.startswith("---"):
                in_section = False
                lines.append("")
                continue
            lines.append(line)

    if not lines:
        return "_æ—  attribution æ•°æ®ï¼ˆéœ€è¦å…ˆè®¾ç½® idea_sourceï¼‰_\n"

    return "\n".join(lines) + "\n"


def generate_forced_questions(days: int, trades_text: str, bias_text: str, kc_text: str) -> str:
    """Generate forced reflection questions based on data."""
    lines = [
        "> ä»¥ä¸‹é—®é¢˜ç”±ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆï¼ŒåŸºäºæœ¬æœŸæ•°æ®ã€‚ä½ å¿…é¡»è®¤çœŸå›ç­”ã€‚",
        "",
        "### Q1: æœ¬æœŸæœ€å¤§çš„ä¸€ä¸ªé”™è¯¯/é—æ†¾æ˜¯ä»€ä¹ˆï¼Ÿ",
        "",
    ]

    # Extract hints from data
    hints = []
    if "ğŸ”´" in bias_text:
        hints.append("BiasEngine æœ‰é«˜ severity alert")
    if "è¿‡æœŸ" in kc_text or "FAIL" in kc_text:
        hints.append("æœ‰ Kill Criteria è¿‡æœŸæˆ–è§¦å‘")
    if hints:
        lines.append(f"_æç¤º: {'; '.join(hints)}_")
    lines.append("")
    lines.append("**ä½ çš„å›ç­”ï¼š** ___")
    lines.append("")
    lines.append("### Q2: å¦‚æœé‡æ¥ï¼Œä½ ä¼šæ”¹å˜ä»€ä¹ˆï¼Ÿ")
    lines.append("**ä½ çš„å›ç­”ï¼š** ___")
    lines.append("")
    lines.append("### Q3: ä½ çš„å“ªä¸ªæŒä»“ä½ æœ€ä¸æƒ³å»æƒ³ï¼Ÿ")
    lines.append("> æç¤ºï¼šè¿™ä¸ªå°±æ˜¯ä½ æœ€è¯¥ç ”ç©¶çš„ã€‚")
    lines.append("**ä½ çš„å›ç­”ï¼š** ___")

    return "\n".join(lines) + "\n"


# â”€â”€ Main Assembly â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def assemble_review(period: str, days: int) -> str:
    """Assemble the full review markdown."""
    today = datetime.now().strftime("%Y-%m-%d")
    period_label = "å‘¨" if period == "week" else "æœˆ"

    # Fetch all data sources (async where possible)
    trades_text = await fetch_trades(days)
    bias_text = await fetch_bias_dashboard(days)

    # Sync sources
    research_text = fetch_obsidian_files(RESEARCH_NOTES, "ç ”ç©¶ç¬”è®°", days)
    earnings_text = fetch_obsidian_files(EARNINGS_ANALYSIS, "è´¢æŠ¥åˆ†æ", days)
    thesis_text = fetch_obsidian_files(THESIS_DIR, "Thesis æ›´æ–°", days)
    zhouhui_text = fetch_obsidian_files(ZHOUHUI, "å‘¨ä¼šè®°å½•", days)
    inbox_text = fetch_obsidian_files(WEEKLY_INBOX, "Weekly Inbox", days)
    podcast_text = fetch_obsidian_files(PODCAST, "æ’­å®¢ç¬”è®°", days)
    kc_text = scan_kill_criteria(days)
    dj_text = fetch_decision_stats(days)
    attribution_text = fetch_attribution_summary()
    estimate_text = fetch_estimate_stats(days)
    peers_text = fetch_peers_reminder(days)
    sizing_text = await compute_sizing_deviations()
    questions_text = generate_forced_questions(days, trades_text, bias_text, kc_text)

    # Build markdown
    md = f"""---
date: {today}
type: {period}-review
period: {days}d
tags: [review, {period}]
---

# {period_label}å›é¡¾: {today}

> å›é¡¾æœŸ: è¿‡å» {days} å¤©

---

## ğŸ“Š äº¤æ˜“è®°å½•

{trades_text}

---

## ğŸ“ ç ”ç©¶æ´»åŠ¨

### ç ”ç©¶ç¬”è®°
{research_text}

### è´¢æŠ¥åˆ†æ
{earnings_text}

### Thesis æ›´æ–°
{thesis_text}

### å‘¨ä¼š
{zhouhui_text}

### Weekly Inbox
{inbox_text}

### æ’­å®¢
{podcast_text}

---

## ğŸ§  è¡Œä¸ºåå·®æ£€æŸ¥

{bias_text}

### ğŸ”´ å¿…é¡»å›ç­”ï¼ˆä¸å…è®¸è·³è¿‡ï¼‰
1. æœ¬æœŸ BiasEngine è§¦å‘äº†å“ªäº› alertï¼Ÿä½ æ˜¯å¦é‡‡å–äº†è¡ŒåŠ¨ï¼Ÿ
   **å›ç­”ï¼š** ___
2. å¦‚æœæ²¡æœ‰è¡ŒåŠ¨ï¼ŒåŸå› æ˜¯ä»€ä¹ˆï¼Ÿ
   **å›ç­”ï¼š** ___
3. ä¸Šä¸€æœŸ review ä¸­ä½ æ‰¿è¯ºçš„æ”¹è¿›ï¼Œæ‰§è¡Œäº†å—ï¼Ÿ
   **å›ç­”ï¼š** ___

---

## âš ï¸ Kill Criteria çŠ¶æ€

{kc_text}

---

## ğŸ“‹ Decision Journal

{dj_text}

---

## ğŸ“Š Research ROI

{attribution_text}

---
"""

    if estimate_text:
        md += f"""## ğŸ“ˆ é¢„æµ‹æ ¡å‡†

{estimate_text}

---
"""

    if sizing_text:
        md += f"""## ğŸ“ Position Sizing å®¡æŸ¥

{sizing_text}

---
"""

    if peers_text:
        md += f"""## ğŸ‘¥ ç«äº‰å¯¹æ‰‹åŠ¨æ€

{peers_text}

---
"""

    md += f"""## ğŸª å¼ºåˆ¶åæ€ï¼ˆä¸å…è®¸å›ç­”"æ²¡æœ‰"ï¼‰

{questions_text}

---

## âœï¸ æœ¬æœŸæ‰¿è¯º

> å†™ä¸‹ä½ ä¸‹ä¸€æœŸè¦æ”¹è¿›çš„ 1-2 ä»¶äº‹ã€‚

**æ‰¿è¯ºï¼š** ___

---
*Generated by review_aggregator.py on {today}*
"""

    return md


async def main():
    parser = argparse.ArgumentParser(description="Investment Review Aggregator")
    parser.add_argument("--period", choices=["week", "month"], default="week")
    parser.add_argument("--days", type=int, default=None, help="Override lookback days")
    parser.add_argument("--dry-run", action="store_true", help="Print to stdout only")
    args = parser.parse_args()

    days = args.days or (7 if args.period == "week" else 30)

    review_md = await assemble_review(args.period, days)

    if args.dry_run:
        print(review_md)
    else:
        today = datetime.now().strftime("%Y-%m-%d")
        filename = f"{today}_{args.period}_review.md"
        filepath = REVIEWS_DIR / filename
        filepath.write_text(review_md, encoding="utf-8")
        print(f"Review saved to: {filepath}")


if __name__ == "__main__":
    asyncio.run(main())
